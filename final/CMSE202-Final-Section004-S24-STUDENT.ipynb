{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: right;\"> &#9989; Ethan Fremder</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSE 202 Final (Section 004 - Spring 2024)\n",
    "\n",
    "The goal of this final is to give you the opportunity to test out some of the skills that you've developed having now finished CMSE 202. In particular, you'll be committing and pushing repository changes to a GitHub repository, working with data to build a network graph, performing regression analysis, and classifying data using a machine learning classifier. You should find that you have all of the skills necessary to complete this exam having completed the second half of CMSE 202!\n",
    "\n",
    "You are encouraged to look through the entire exam before you get started so that you can appropriately budget your time and understand the broad goals of the exam. Once you've read through it, you'll probably want to make sure you do Part 1 first to ensure that your GitHub repository is working correctly. Let your instructor know right away if you run into issues!\n",
    "\n",
    "The exam is set up so that even if you get stuck on one part there are opportunities to get points on the other parts, so consider jumping ahead if you feel like you aren't making progress and then come back later if you have time.\n",
    "\n",
    "**Important note about using online resources**: This exam is \"open internet\". That means that you can look up documentation, google how to accomplish certain Python tasks, etc. Being able to effectively use the internet for computational modeling and data science is a very important skill, so we want to make sure you have the opportunity to exercise that skill. You can also use _your version_ of past CMSE 202 assignments and the CMSE 202 course materials as a resource! **However: The use of any person-to-person communication software or generative AI tools is absolutely not acceptable.** If you are seen accessing your email, using a collaborative cloud storage or document software (e.g. Slack, Google Documents), or generative AIs (e.g. ChatGPT), you will be at risk for receiving a zero on the exam.\n",
    "\n",
    "**Keep your eyes on your screen!** Unfortunately, there isn't enough space in the room for everyone to sit at their own table so please do your best to keep your eyes on your own screen. This exam is designed to give *you* the opportunity to show the instructor what you can do and you should hold yourself accountable for maintaining a high level of academic integrity. If any of the instructors observe suspicious behavior, you will, again, risk receiving a zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 0: Academic integrity statement\n",
    "\n",
    "Read the following statement and edit the markdown text to put your name in the statement. This is your commitment to doing your own authentic work on this exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I, **ETHAN FREMDER**, affirm that this exam represents my own authetic work, without the use of any unpermitted aids or resources or person-to-person communication. I understand that this exam an an opportunity to showcase my own progress in developing and improving my computational skills and have done my best to demonstrate those skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Add to your Git repository to track your progress on your exam (4 points)\n",
    "\n",
    "Before you get to far along in the exam, you're going to add it to the `cmse202-s24-turnin` repository you created in class so that you can track your progress on the exam and preserve the final version that you turn in. **Make sure to pull your most recent respository before beginning**. In order to do this you need to\n",
    "\n",
    "**&#9989; Do the following**:\n",
    "\n",
    "1. Navigate to your `cmse202-s24-turnin` repository and create a new directory called `final`.\n",
    "2. Move this notebook into that **new directory** in your repository, then **add it and commit it to your repository**.\n",
    "1. Finally, to test that everything is working, \"git push\" the file so that it ends up in your GitHub repository.\n",
    "\n",
    "**Important**: Double check you've added your Professor and your TA as collaborators to your \"turnin\" respository (you should have done this in the previous homework assignment).\n",
    "\n",
    "**Also important**: Make sure that the version of this notebook that you are working on is the same one that you just added to your repository! If you are working on a different copy of the noteobok, **none of your changes will be tracked**!\n",
    "\n",
    "If everything went as intended, the file should now show up on your GitHub account in the \"`cmse202-s24-turnin`\" repository inside the `final` directory that you just created. Periodically, **you'll be asked to commit your changes to the repository and push them to the remote GitHub location**. Of course, you can always commit your changes more often than that, if you wish.  It can be good to get into a habit of committing your changes any time you make a significant modification, or when you stop working on the project for a bit.\n",
    "\n",
    "&#9989; **Do this**: Before you move on, put the command that your instructor should run to clone your repository in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "# git clone https://github.com/ethanfremder/CMSE202-f23-turnin\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Generate a network graph from data (16 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exam, we will look at Facebook groups in Tennessee. Some of these groups are very connected (share members with many groups), while others are not very connected (share members with few other groups). We will model these group connections as an undirected graph. Every node will be a Facebook group and there will be an edge between two groups if they share at least one member. The dataset originally comes from [here](https://www.kaggle.com/datasets/stkbailey/nashville-meetup?select=group-edges.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.1 (3 points)**: To get started, **download the `.csv` file and place it in the same directory as your notebook**, then **read in the `FacebookGroupNetwork.csv` dataset** and finally **display the first few rows of the data**. You can use **Pandas** for this task or any other Python tool you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nashville CocoaHeads</td>\n",
       "      <td>Nash.rb</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nashville CocoaHeads</td>\n",
       "      <td>Nashville Christian Technologists and Entrepre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nashville CocoaHeads</td>\n",
       "      <td>Stepping Out Social Dance Meetup</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nashville CocoaHeads</td>\n",
       "      <td>NashReact</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nashville CocoaHeads</td>\n",
       "      <td>WordPress Nashville</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 group1                                             group2  \\\n",
       "0  Nashville CocoaHeads                                            Nash.rb   \n",
       "1  Nashville CocoaHeads  Nashville Christian Technologists and Entrepre...   \n",
       "2  Nashville CocoaHeads                   Stepping Out Social Dance Meetup   \n",
       "3  Nashville CocoaHeads                                          NashReact   \n",
       "4  Nashville CocoaHeads                                WordPress Nashville   \n",
       "\n",
       "   weight  \n",
       "0       2  \n",
       "1       1  \n",
       "2       1  \n",
       "3       2  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "import pandas as pd\n",
    "facebook = pd.read_csv('FacebookGroupNetwork.csv')\n",
    "facebook.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see three columns: `group1`, `group2`, and `weight`. We are going to ignore `weight` in this exam (it is the number of members shared between groups). `group1` and `group2` are the two (unique) group names that share some number of members. We will now create a `networkx` graph using this dataset.\n",
    "\n",
    "&#9989; **Question 2.2 (4 points)**: **Create an undirected `networkx` graph** (you can call it `G`). Make sure it is an undirected graph. Then, iterate over the dataset and **add edges between the `group1` and `group2` groups** on each line. Ignore the weights. (The resulting graph should now have an edge per entry in the dataset and the set of all group names should be the set of all nodes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here\n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "for i in range(len(facebook)):\n",
    "    G.add_edge(facebook['group1'][i],facebook['group2'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize the graph.\n",
    "\n",
    "&#9989; **Question 2.3 (5 points)**: Create a large figure for drawing the graph using something like `plt.figure(figsize=(20,20))`. Then, draw the graph using `networkx`. Make sure that when drawing your graph, you accomplish the following:\n",
    "1. The `\"Data Science Nashville\"` node should be colored red (it may be hard to see since this will appear in the middle since it is a highly connected group).\n",
    "2. The group `\"Nashville Squash Meetup\"` should be colored blue (this will appear at the edge with only one connection).\n",
    "3. All of the other groups should have a third, different color.\n",
    "\n",
    "To recap, you should have **three** different colors in your graph, one for `\"Data Science Nashville\"`, one for `\"Nashville Squash Meetup\"`, and one for all of the other groups.\n",
    "\n",
    "(Partial credit if you generate the graph but the colors are not set as described.)\n",
    "\n",
    "**Note**: this will be a very crowded graph because it's a complex, heavily-interconnected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'c' argument must be a color, a sequence of colors, or a sequence of numbers, not {'Data Science Nashville': 'red', 'Nashville Squash Meetup': 'blue'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4439\u001b[0m, in \u001b[0;36mAxes._parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# Is 'c' acceptable as PathCollection facecolors?\u001b[39;00m\n\u001b[0;32m-> 4439\u001b[0m     colors \u001b[38;5;241m=\u001b[39m mcolors\u001b[38;5;241m.\u001b[39mto_rgba_array(c)\n\u001b[1;32m   4440\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/colors.py:487\u001b[0m, in \u001b[0;36mto_rgba_array\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     rgba \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([to_rgba(cc) \u001b[38;5;28;01mfor\u001b[39;00m cc \u001b[38;5;129;01min\u001b[39;00m c])\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/colors.py:487\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     rgba \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([to_rgba(cc) \u001b[38;5;28;01mfor\u001b[39;00m cc \u001b[38;5;129;01min\u001b[39;00m c])\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/colors.py:299\u001b[0m, in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rgba \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Suppress exception chaining of cache lookup failure.\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m     rgba \u001b[38;5;241m=\u001b[39m _to_rgba_no_colorcycle(c, alpha)\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/colors.py:374\u001b[0m, in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m c, c, c, alpha \u001b[38;5;28;01mif\u001b[39;00m alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.\u001b[39m\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid RGBA argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_c\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# turn 2-D array into 1-D array\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid RGBA argument: 'Data Science Nashville'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m      4\u001b[0m node_colors \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData Science Nashville\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNashville Squash Meetup\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m----> 5\u001b[0m nx\u001b[38;5;241m.\u001b[39mdraw(G, node_color \u001b[38;5;241m=\u001b[39m node_colors)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/networkx/drawing/nx_pylab.py:121\u001b[0m, in \u001b[0;36mdraw\u001b[0;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m    119\u001b[0m     kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds\n\u001b[0;32m--> 121\u001b[0m draw_networkx(G, pos\u001b[38;5;241m=\u001b[39mpos, ax\u001b[38;5;241m=\u001b[39max, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    122\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_axis_off()\n\u001b[1;32m    123\u001b[0m plt\u001b[38;5;241m.\u001b[39mdraw_if_interactive()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/networkx/drawing/nx_pylab.py:303\u001b[0m, in \u001b[0;36mdraw_networkx\u001b[0;34m(G, pos, arrows, with_labels, **kwds)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     pos \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mdrawing\u001b[38;5;241m.\u001b[39mspring_layout(G)  \u001b[38;5;66;03m# default to spring layout\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m draw_networkx_nodes(G, pos, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnode_kwds)\n\u001b[1;32m    304\u001b[0m draw_networkx_edges(G, pos, arrows\u001b[38;5;241m=\u001b[39marrows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39medge_kwds)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_labels:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/networkx/drawing/nx_pylab.py:433\u001b[0m, in \u001b[0;36mdraw_networkx_nodes\u001b[0;34m(G, pos, nodelist, node_size, node_color, node_shape, alpha, cmap, vmin, vmax, ax, linewidths, edgecolors, label, margins)\u001b[0m\n\u001b[1;32m    430\u001b[0m     node_color \u001b[38;5;241m=\u001b[39m apply_alpha(node_color, alpha, nodelist, cmap, vmin, vmax)\n\u001b[1;32m    431\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m node_collection \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mscatter(\n\u001b[1;32m    434\u001b[0m     xy[:, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    435\u001b[0m     xy[:, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    436\u001b[0m     s\u001b[38;5;241m=\u001b[39mnode_size,\n\u001b[1;32m    437\u001b[0m     c\u001b[38;5;241m=\u001b[39mnode_color,\n\u001b[1;32m    438\u001b[0m     marker\u001b[38;5;241m=\u001b[39mnode_shape,\n\u001b[1;32m    439\u001b[0m     cmap\u001b[38;5;241m=\u001b[39mcmap,\n\u001b[1;32m    440\u001b[0m     vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[1;32m    441\u001b[0m     vmax\u001b[38;5;241m=\u001b[39mvmax,\n\u001b[1;32m    442\u001b[0m     alpha\u001b[38;5;241m=\u001b[39malpha,\n\u001b[1;32m    443\u001b[0m     linewidths\u001b[38;5;241m=\u001b[39mlinewidths,\n\u001b[1;32m    444\u001b[0m     edgecolors\u001b[38;5;241m=\u001b[39medgecolors,\n\u001b[1;32m    445\u001b[0m     label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[1;32m    446\u001b[0m )\n\u001b[1;32m    447\u001b[0m ax\u001b[38;5;241m.\u001b[39mtick_params(\n\u001b[1;32m    448\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    449\u001b[0m     which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m     labelleft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    454\u001b[0m )\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m margins \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1444\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4602\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edgecolors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4600\u001b[0m     orig_edgecolor \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   4601\u001b[0m c, colors, edgecolors \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m-> 4602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_scatter_color_args(\n\u001b[1;32m   4603\u001b[0m         c, edgecolors, kwargs, x\u001b[38;5;241m.\u001b[39msize,\n\u001b[1;32m   4604\u001b[0m         get_next_color_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_patches_for_fill\u001b[38;5;241m.\u001b[39mget_next_color)\n\u001b[1;32m   4606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plotnonfinite \u001b[38;5;129;01mand\u001b[39;00m colors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4607\u001b[0m     c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mmasked_invalid(c)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:4448\u001b[0m, in \u001b[0;36mAxes._parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4445\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m invalid_shape_exception(c\u001b[38;5;241m.\u001b[39msize, xsize) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   4446\u001b[0m         \u001b[38;5;66;03m# Both the mapping *and* the RGBA conversion failed: pretty\u001b[39;00m\n\u001b[1;32m   4447\u001b[0m         \u001b[38;5;66;03m# severe failure => one may appreciate a verbose feedback.\u001b[39;00m\n\u001b[0;32m-> 4448\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4449\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument must be a color, a sequence of colors, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4450\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a sequence of numbers, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   4451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(colors) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, xsize):\n\u001b[1;32m   4453\u001b[0m         \u001b[38;5;66;03m# NB: remember that a single color is also acceptable.\u001b[39;00m\n\u001b[1;32m   4454\u001b[0m         \u001b[38;5;66;03m# Besides *colors* will be an empty array if c == 'none'.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: 'c' argument must be a color, a sequence of colors, or a sequence of numbers, not {'Data Science Nashville': 'red', 'Nashville Squash Meetup': 'blue'}"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Put your code here\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,20))\n",
    "node_colors = {'Data Science Nashville': 'red', \"Nashville Squash Meetup\": 'blue'}\n",
    "nx.draw(G, node_color = node_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using **only the graph you created (not the initial row-based data)**, answer the following questions.\n",
    "\n",
    "You may find it useful to review the \"Methods\" section of the [networkx Graph documentation](https://networkx.org/documentation/stable/reference/classes/graph.html#methods).\n",
    "\n",
    "&#9989; **Question 2.4 (1 point)** What is the total number of groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.5 (1 point)** With how many connections does `\"Code for Nashville\"` have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "G.degree('Code for Nashville')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.6 (1 point)** True or False?: `\"Nashville Online Entrepreneurs\"` has a connection with `\"Nashville Tennis Lessons\"`. Use a graph method to determine this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "G.number_of_edges('Nashville Online Entrepreneurs',\"Nashville Tennis Lessons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False, there is no edge between the two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.7 (1 point)** Using the relevant `networkx` function (consult the documentation/internet resources), **find the \"shortest path\" between `\"All Things German Book Club\"` and `\"Data Science Nashville\"`**. The two groups are not directly connected, but you should be able to determine a set of nodes to \"traverse\" to get from one group to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All Things German Book Club',\n",
       " 'All Things Gardening',\n",
       " 'Data Science Nashville']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "nx.shortest_path(G,\"All Things German Book Club\",\"Data Science Nashville\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"**Committing Part 2**\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Perform a regression analysis on data (16 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be looking at a dataset of housing prices in Boston. \n",
    "\n",
    "&#9989; **Question 3.1 (2 points)**: To get started, **download the `BostonHousing.csv` file and place it in the same directory as your notebook**, then **read in the `BostonHousing.csv` dataset** and finally **display the first few rows of the data**. You can use **Pandas** for this task or any other Python tool you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        b  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "boston = pd.read_csv('BostonHousing.csv')\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a list of house median values with several different pieces of information about them, such as the crime rate, age, property tax rate, etc.\n",
    "\n",
    "You will be trying to predict `medv` (median value) using linear regression using a subset of the other features.\n",
    "\n",
    "&#9989; **Question 3.2 (3 points)**: Create two arrays and/or dataframes from the data you just loaded, one of them called `y`, the other one called `X`. `y` should **only** include the `medv` column, while `X` should include **all the remaining columns**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here\n",
    "y = boston['medv']\n",
    "X = boston.drop(['medv'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the labels and features to fit, we will use the `statsmodels` `OLS` model to fit it. \n",
    "\n",
    "&#9989; **Question 3.3 (2 point)**: Before we do this, **add a column of constants (set to 1.0) to the `X`**. There is a `statsmodel` function you saw in class that allows you to do that. Call this new data structure `X_const`. (If you cannot figure this out, you can use `X` instead of `X_const` for the next questions.) Display the first 5 rows of `X_const` to make sure the new column exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   const     crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
       "0    1.0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
       "1    1.0  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
       "2    1.0  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n",
       "3    1.0  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
       "4    1.0  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
       "\n",
       "   ptratio       b  lstat  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_const = sm.add_constant(X)\n",
    "X_const.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will perform the actual fit.\n",
    "\n",
    "&#9989; **Question 3.4 (3 points)**: Using `statsmodels` `OLS`, perform a fit using `y` (containing `medv`) as the quantity to fit (y) and fit it to the `X_const` (X). Once the fit is done print the fit `summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.741</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.734</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   108.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Apr 2024</td> <th>  Prob (F-statistic):</th> <td>6.72e-135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:48:36</td>     <th>  Log-Likelihood:    </th> <td> -1498.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3026.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   492</td>      <th>  BIC:               </th> <td>   3085.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   36.4595</td> <td>    5.103</td> <td>    7.144</td> <td> 0.000</td> <td>   26.432</td> <td>   46.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>crim</th>    <td>   -0.1080</td> <td>    0.033</td> <td>   -3.287</td> <td> 0.001</td> <td>   -0.173</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zn</th>      <td>    0.0464</td> <td>    0.014</td> <td>    3.382</td> <td> 0.001</td> <td>    0.019</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>indus</th>   <td>    0.0206</td> <td>    0.061</td> <td>    0.334</td> <td> 0.738</td> <td>   -0.100</td> <td>    0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chas</th>    <td>    2.6867</td> <td>    0.862</td> <td>    3.118</td> <td> 0.002</td> <td>    0.994</td> <td>    4.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nox</th>     <td>  -17.7666</td> <td>    3.820</td> <td>   -4.651</td> <td> 0.000</td> <td>  -25.272</td> <td>  -10.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rm</th>      <td>    3.8099</td> <td>    0.418</td> <td>    9.116</td> <td> 0.000</td> <td>    2.989</td> <td>    4.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>     <td>    0.0007</td> <td>    0.013</td> <td>    0.052</td> <td> 0.958</td> <td>   -0.025</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dis</th>     <td>   -1.4756</td> <td>    0.199</td> <td>   -7.398</td> <td> 0.000</td> <td>   -1.867</td> <td>   -1.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rad</th>     <td>    0.3060</td> <td>    0.066</td> <td>    4.613</td> <td> 0.000</td> <td>    0.176</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tax</th>     <td>   -0.0123</td> <td>    0.004</td> <td>   -3.280</td> <td> 0.001</td> <td>   -0.020</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ptratio</th> <td>   -0.9527</td> <td>    0.131</td> <td>   -7.283</td> <td> 0.000</td> <td>   -1.210</td> <td>   -0.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>b</th>       <td>    0.0093</td> <td>    0.003</td> <td>    3.467</td> <td> 0.001</td> <td>    0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lstat</th>   <td>   -0.5248</td> <td>    0.051</td> <td>  -10.347</td> <td> 0.000</td> <td>   -0.624</td> <td>   -0.425</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>178.041</td> <th>  Durbin-Watson:     </th> <td>   1.078</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 783.126</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.521</td>  <th>  Prob(JB):          </th> <td>8.84e-171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.281</td>  <th>  Cond. No.          </th> <td>1.51e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.51e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       medv       & \\textbf{  R-squared:         } &     0.741   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.734   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     108.1   \\\\\n",
       "\\textbf{Date:}             & Thu, 18 Apr 2024 & \\textbf{  Prob (F-statistic):} & 6.72e-135   \\\\\n",
       "\\textbf{Time:}             &     10:48:36     & \\textbf{  Log-Likelihood:    } &   -1498.8   \\\\\n",
       "\\textbf{No. Observations:} &         506      & \\textbf{  AIC:               } &     3026.   \\\\\n",
       "\\textbf{Df Residuals:}     &         492      & \\textbf{  BIC:               } &     3085.   \\\\\n",
       "\\textbf{Df Model:}         &          13      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                 & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}   &      36.4595  &        5.103     &     7.144  &         0.000        &       26.432    &       46.487     \\\\\n",
       "\\textbf{crim}    &      -0.1080  &        0.033     &    -3.287  &         0.001        &       -0.173    &       -0.043     \\\\\n",
       "\\textbf{zn}      &       0.0464  &        0.014     &     3.382  &         0.001        &        0.019    &        0.073     \\\\\n",
       "\\textbf{indus}   &       0.0206  &        0.061     &     0.334  &         0.738        &       -0.100    &        0.141     \\\\\n",
       "\\textbf{chas}    &       2.6867  &        0.862     &     3.118  &         0.002        &        0.994    &        4.380     \\\\\n",
       "\\textbf{nox}     &     -17.7666  &        3.820     &    -4.651  &         0.000        &      -25.272    &      -10.262     \\\\\n",
       "\\textbf{rm}      &       3.8099  &        0.418     &     9.116  &         0.000        &        2.989    &        4.631     \\\\\n",
       "\\textbf{age}     &       0.0007  &        0.013     &     0.052  &         0.958        &       -0.025    &        0.027     \\\\\n",
       "\\textbf{dis}     &      -1.4756  &        0.199     &    -7.398  &         0.000        &       -1.867    &       -1.084     \\\\\n",
       "\\textbf{rad}     &       0.3060  &        0.066     &     4.613  &         0.000        &        0.176    &        0.436     \\\\\n",
       "\\textbf{tax}     &      -0.0123  &        0.004     &    -3.280  &         0.001        &       -0.020    &       -0.005     \\\\\n",
       "\\textbf{ptratio} &      -0.9527  &        0.131     &    -7.283  &         0.000        &       -1.210    &       -0.696     \\\\\n",
       "\\textbf{b}       &       0.0093  &        0.003     &     3.467  &         0.001        &        0.004    &        0.015     \\\\\n",
       "\\textbf{lstat}   &      -0.5248  &        0.051     &   -10.347  &         0.000        &       -0.624    &       -0.425     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 178.041 & \\textbf{  Durbin-Watson:     } &     1.078  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &   783.126  \\\\\n",
       "\\textbf{Skew:}          &   1.521 & \\textbf{  Prob(JB):          } & 8.84e-171  \\\\\n",
       "\\textbf{Kurtosis:}      &   8.281 & \\textbf{  Cond. No.          } &  1.51e+04  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.51e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   medv   R-squared:                       0.741\n",
       "Model:                            OLS   Adj. R-squared:                  0.734\n",
       "Method:                 Least Squares   F-statistic:                     108.1\n",
       "Date:                Thu, 18 Apr 2024   Prob (F-statistic):          6.72e-135\n",
       "Time:                        10:48:36   Log-Likelihood:                -1498.8\n",
       "No. Observations:                 506   AIC:                             3026.\n",
       "Df Residuals:                     492   BIC:                             3085.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         36.4595      5.103      7.144      0.000      26.432      46.487\n",
       "crim          -0.1080      0.033     -3.287      0.001      -0.173      -0.043\n",
       "zn             0.0464      0.014      3.382      0.001       0.019       0.073\n",
       "indus          0.0206      0.061      0.334      0.738      -0.100       0.141\n",
       "chas           2.6867      0.862      3.118      0.002       0.994       4.380\n",
       "nox          -17.7666      3.820     -4.651      0.000     -25.272     -10.262\n",
       "rm             3.8099      0.418      9.116      0.000       2.989       4.631\n",
       "age            0.0007      0.013      0.052      0.958      -0.025       0.027\n",
       "dis           -1.4756      0.199     -7.398      0.000      -1.867      -1.084\n",
       "rad            0.3060      0.066      4.613      0.000       0.176       0.436\n",
       "tax           -0.0123      0.004     -3.280      0.001      -0.020      -0.005\n",
       "ptratio       -0.9527      0.131     -7.283      0.000      -1.210      -0.696\n",
       "b              0.0093      0.003      3.467      0.001       0.004       0.015\n",
       "lstat         -0.5248      0.051    -10.347      0.000      -0.624      -0.425\n",
       "==============================================================================\n",
       "Omnibus:                      178.041   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126\n",
       "Skew:                           1.521   Prob(JB):                    8.84e-171\n",
       "Kurtosis:                       8.281   Cond. No.                     1.51e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.51e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "model = sm.OLS(y, X_const)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.5 (2 points)**: Which 2 features would you say contributes the least to the fit result and/or is the least significant? Make sure to justify your answer with a sentence or two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> The two features that are the least significant are 'indus' and 'age'. I can tell as the p value is very high and not under the 0.05 threshold that we want "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.6 (2 points)**: Now **run the fit again, but with the \"least important\" features you identified in Q3.5 removed**. Make sure your new features still include the `constant` column (unless that happens to be one of the least important features). **Print the fit `summary()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.741</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.735</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   128.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Apr 2024</td> <th>  Prob (F-statistic):</th> <td>5.54e-137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:53:49</td>     <th>  Log-Likelihood:    </th> <td> -1498.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3022.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   494</td>      <th>  BIC:               </th> <td>   3072.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   36.3411</td> <td>    5.067</td> <td>    7.171</td> <td> 0.000</td> <td>   26.385</td> <td>   46.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>crim</th>    <td>   -0.1084</td> <td>    0.033</td> <td>   -3.307</td> <td> 0.001</td> <td>   -0.173</td> <td>   -0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zn</th>      <td>    0.0458</td> <td>    0.014</td> <td>    3.390</td> <td> 0.001</td> <td>    0.019</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chas</th>    <td>    2.7187</td> <td>    0.854</td> <td>    3.183</td> <td> 0.002</td> <td>    1.040</td> <td>    4.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nox</th>     <td>  -17.3760</td> <td>    3.535</td> <td>   -4.915</td> <td> 0.000</td> <td>  -24.322</td> <td>  -10.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rm</th>      <td>    3.8016</td> <td>    0.406</td> <td>    9.356</td> <td> 0.000</td> <td>    3.003</td> <td>    4.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dis</th>     <td>   -1.4927</td> <td>    0.186</td> <td>   -8.037</td> <td> 0.000</td> <td>   -1.858</td> <td>   -1.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rad</th>     <td>    0.2996</td> <td>    0.063</td> <td>    4.726</td> <td> 0.000</td> <td>    0.175</td> <td>    0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tax</th>     <td>   -0.0118</td> <td>    0.003</td> <td>   -3.493</td> <td> 0.001</td> <td>   -0.018</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ptratio</th> <td>   -0.9465</td> <td>    0.129</td> <td>   -7.334</td> <td> 0.000</td> <td>   -1.200</td> <td>   -0.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>b</th>       <td>    0.0093</td> <td>    0.003</td> <td>    3.475</td> <td> 0.001</td> <td>    0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lstat</th>   <td>   -0.5226</td> <td>    0.047</td> <td>  -11.019</td> <td> 0.000</td> <td>   -0.616</td> <td>   -0.429</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>178.430</td> <th>  Durbin-Watson:     </th> <td>   1.078</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 787.785</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.523</td>  <th>  Prob(JB):          </th> <td>8.60e-172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.300</td>  <th>  Cond. No.          </th> <td>1.47e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.47e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       medv       & \\textbf{  R-squared:         } &     0.741   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.735   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     128.2   \\\\\n",
       "\\textbf{Date:}             & Thu, 18 Apr 2024 & \\textbf{  Prob (F-statistic):} & 5.54e-137   \\\\\n",
       "\\textbf{Time:}             &     10:53:49     & \\textbf{  Log-Likelihood:    } &   -1498.9   \\\\\n",
       "\\textbf{No. Observations:} &         506      & \\textbf{  AIC:               } &     3022.   \\\\\n",
       "\\textbf{Df Residuals:}     &         494      & \\textbf{  BIC:               } &     3072.   \\\\\n",
       "\\textbf{Df Model:}         &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                 & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}   &      36.3411  &        5.067     &     7.171  &         0.000        &       26.385    &       46.298     \\\\\n",
       "\\textbf{crim}    &      -0.1084  &        0.033     &    -3.307  &         0.001        &       -0.173    &       -0.044     \\\\\n",
       "\\textbf{zn}      &       0.0458  &        0.014     &     3.390  &         0.001        &        0.019    &        0.072     \\\\\n",
       "\\textbf{chas}    &       2.7187  &        0.854     &     3.183  &         0.002        &        1.040    &        4.397     \\\\\n",
       "\\textbf{nox}     &     -17.3760  &        3.535     &    -4.915  &         0.000        &      -24.322    &      -10.430     \\\\\n",
       "\\textbf{rm}      &       3.8016  &        0.406     &     9.356  &         0.000        &        3.003    &        4.600     \\\\\n",
       "\\textbf{dis}     &      -1.4927  &        0.186     &    -8.037  &         0.000        &       -1.858    &       -1.128     \\\\\n",
       "\\textbf{rad}     &       0.2996  &        0.063     &     4.726  &         0.000        &        0.175    &        0.424     \\\\\n",
       "\\textbf{tax}     &      -0.0118  &        0.003     &    -3.493  &         0.001        &       -0.018    &       -0.005     \\\\\n",
       "\\textbf{ptratio} &      -0.9465  &        0.129     &    -7.334  &         0.000        &       -1.200    &       -0.693     \\\\\n",
       "\\textbf{b}       &       0.0093  &        0.003     &     3.475  &         0.001        &        0.004    &        0.015     \\\\\n",
       "\\textbf{lstat}   &      -0.5226  &        0.047     &   -11.019  &         0.000        &       -0.616    &       -0.429     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 178.430 & \\textbf{  Durbin-Watson:     } &     1.078  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &   787.785  \\\\\n",
       "\\textbf{Skew:}          &   1.523 & \\textbf{  Prob(JB):          } & 8.60e-172  \\\\\n",
       "\\textbf{Kurtosis:}      &   8.300 & \\textbf{  Cond. No.          } &  1.47e+04  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.47e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   medv   R-squared:                       0.741\n",
       "Model:                            OLS   Adj. R-squared:                  0.735\n",
       "Method:                 Least Squares   F-statistic:                     128.2\n",
       "Date:                Thu, 18 Apr 2024   Prob (F-statistic):          5.54e-137\n",
       "Time:                        10:53:49   Log-Likelihood:                -1498.9\n",
       "No. Observations:                 506   AIC:                             3022.\n",
       "Df Residuals:                     494   BIC:                             3072.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         36.3411      5.067      7.171      0.000      26.385      46.298\n",
       "crim          -0.1084      0.033     -3.307      0.001      -0.173      -0.044\n",
       "zn             0.0458      0.014      3.390      0.001       0.019       0.072\n",
       "chas           2.7187      0.854      3.183      0.002       1.040       4.397\n",
       "nox          -17.3760      3.535     -4.915      0.000     -24.322     -10.430\n",
       "rm             3.8016      0.406      9.356      0.000       3.003       4.600\n",
       "dis           -1.4927      0.186     -8.037      0.000      -1.858      -1.128\n",
       "rad            0.2996      0.063      4.726      0.000       0.175       0.424\n",
       "tax           -0.0118      0.003     -3.493      0.001      -0.018      -0.005\n",
       "ptratio       -0.9465      0.129     -7.334      0.000      -1.200      -0.693\n",
       "b              0.0093      0.003      3.475      0.001       0.004       0.015\n",
       "lstat         -0.5226      0.047    -11.019      0.000      -0.616      -0.429\n",
       "==============================================================================\n",
       "Omnibus:                      178.430   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              787.785\n",
       "Skew:                           1.523   Prob(JB):                    8.60e-172\n",
       "Kurtosis:                       8.300   Cond. No.                     1.47e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.47e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "X = X.drop(['indus','age'],axis=1)\n",
    "X_cnst = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_cnst)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.7 (2 points)**: Comment on the difference in fit quality between the two fits you just performed. Is one much better or worse than the other? Is the difference what you expected? Explain how you judged the quality given the fit statistics you printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Both models had the same rsquared value of 0.741. However, because the second model had less features and still put up the same rsquared value, the second model is better than the first. This makes sense as the features we took out had very very little significance so it wouldn't change the model that much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"**Committing Part 3**\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Perform a support vector machine (SVM) classification on data (16 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the exam, you will be using a dataset that records metrics about songs and the goal is to predict the genre of the song. \n",
    "\n",
    "&#9989; **Question 4.1 (1 point)**: To get started, **download the `music_genre_classifier.csv` file and place it in the same directory as your notebook**, then **read in the `music_genre_classifier.csv` dataset** and finally **display the first few rows of the data**. You can use **Pandas** for this task or any other Python tool you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to perform **classification**. We will try to see if we can classify the genre using the metrics that are given. We will need perform a train-test split on the data first.\n",
    "\n",
    "&#9989; **Question 4.2 (3 points)**: **Create two data structures** (e.g. dataframes) from your table: one called `labels` containing **only** the values from the `music_genre` column and one called `features` containing **everything but** the `music_genre` column.\n",
    "\n",
    "Then, perform a **train-test-split** using functions we used in class. Use a `train_size` of `0.75` and `random_state` of `10`. You should now have a training and a testing set with \"labels\" and \"features\" each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.3 (6 points)**: **Fit an SVM classifier (using the `sklearn` `SVC` class) to the dataset.** Use a `linear` kernel and set the hyper-parameter `C=1.0`. Then **fit your *training* set** and use the resulting fit to **predict your the *testing* set** so you get predicted labels for the testing set. Finally, print the fit statistics using `confusion_matrix` and `classification_report` (if you prefer the visual plot version of the confusion matrix, you can use `ConfusionMatrixDisplay` from `sklearn.metrics` instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.4 (3 points)**: Interpret the output of your classification report and the confusion matrix by answering these three questions (provide at least 1 or 2 sentences each for full credit): \n",
    "* Explain in a few sentences what you observe in the confusion matrix. \n",
    "* Would you consider this a good or a bad classifier?\n",
    "* Which quantity from the classification report did you use to make this judgement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Do This - Erase the contents of this cell an put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.5 (3 points)**: We have been using machine learning \"jargon\" in this section and in class. In a few sentences each explain the following concepts:\n",
    "* What are \"labels\" and \"features\"?\n",
    "* Why do we need \"training sets\" and \"testing sets\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Do This - Erase the contents of this cell an put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"**Committing Part 4**\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're done! Congrats on finishing your CMSE 202 Final!\n",
    "\n",
    "Make sure all of your changes to your repository are committed and pushed to GitHub. Also upload a copy of this notebook to the dropbox on D2L in case something went wrong with your repository or if you couldn't get the repository to work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
